# DeepSeekçš„å¼€æºå‘¨ï¼Œå¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®ï¼Ÿæœ‰å“ªäº›é»‘ç§‘æŠ€



## ç¬¬ä¸€å¤©-FlashMLA

Day 1 of #OpenSourceWeek: FlashMLA

Honored to share FlashMLA - our efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences and now in production.

âœ… BF16 support
âœ… Paged KV cache (block size 64)
âš¡ 3000 GB/s memory-bound & 580 TFLOPS compute-bound on H800

ğŸ”— Explore on GitHub: https://github.com/deepseek-ai/FlashMLA



## ç¬¬äºŒå¤©-DeepEP

Day 2 of #OpenSourceWeek: DeepEP

Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.

âœ… Efficient and optimized all-to-all communication
âœ… Both intranode and internode support with NVLink and RDMA
âœ… High-throughput kernels for training and inference prefilling
âœ… Low-latency kernels for inference decoding
âœ… Native FP8 dispatch support
âœ… Flexible GPU resource control for computation-communication overlapping

ğŸ”— GitHub: https://github.com/deepseek-ai/DeepEP

## ç¬¬ä¸‰å¤©-DeepGEMM

ğŸš€ Day 3 of #OpenSourceWeek: DeepGEMM

Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.

âš¡ Up to 1350+ FP8 TFLOPS on Hopper GPUs
âœ… No heavy dependency, as clean as a tutorial
âœ… Fully Just-In-Time compiled
âœ… Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes
âœ… Supports dense layout and two MoE layouts

ğŸ”— GitHub: https://github.com/deepseek-ai/DeepGEMM



## ç¬¬å››å¤©-Optimized Parallelism Strategies

ğŸš€ Day 4 of #OpenSourceWeek: Optimized Parallelism Strategies

âœ… DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.
ğŸ”— https://github.com/deepseek-ai/DualPipe

âœ… EPLB - an expert-parallel load balancer for V3/R1.
ğŸ”— https://github.com/deepseek-ai/eplb

ğŸ“Š Analyze computation-communication overlap in V3/R1.
ğŸ”— https://github.com/deepseek-ai/profile-data



## ç¬¬äº”å¤©-3FS, Thruster for All DeepSeek Data Access

ğŸš€ Day 5 of #OpenSourceWeek: 3FS, Thruster for All DeepSeek Data Access

Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.

âš¡ 6.6 TiB/s aggregate read throughput in a 180-node cluster
âš¡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster
âš¡ 40+ GiB/s peak throughput per client node for KVCache lookup
ğŸ§¬ Disaggregated architecture with strong consistency semantics
âœ… Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search & KVCache lookups for inference in V3/R1

ğŸ“¥ 3FS â†’ https://github.com/deepseek-ai/3FS
â›² Smallpond - data processing framework on 3FS â†’ https://github.com/deepseek-ai/smallpond



## ç¬¬å…­å¤©-DeepSeek-V3/R1 Inference System Overview

Day 6 of #OpenSourceWeek: One More Thing â€“ DeepSeek-V3/R1 Inference System Overview

Optimized throughput and latency via:
ğŸ”§ Cross-node EP-powered batch scaling
ğŸ”„ Computation-communication overlap
âš–ï¸ Load balancing

Statistics of DeepSeek's Online Service:
âš¡ 73.7k/14.8k input/output tokens per second per H800 node
ğŸš€ Cost profit margin 545%

ğŸ’¡ We hope this week's insights offer value to the community and contribute to our shared AGI goals.