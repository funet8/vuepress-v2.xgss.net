# 零代码封神！Dify 爬虫工作流：批量抓取公众号文章，一键筑牢 AI 知识库（保姆级实操指南）

在 AI 创作与业务赋能的场景中，知识库是核心基石 —— 它既能解决 AI 幻觉问题，让输出内容精准可控，又能为垂直领域创作提供专业素材支撑。但多数人面临的共性痛点是：手动录入知识库内容效率低下，而传统工具又受限于功能边界，难以适配业务级需求。

今天，我们就带来一套 **Dify + 爬虫** 的完美解决方案，无需复杂编程，从零搭建自动化工作流，实现公众号文章批量抓取、智能处理并一键存入 AI 知识库。无论你是教培、自媒体还是企业运营，这套方法都能让知识库搭建效率翻倍，亲测有效！



## 一、为什么要做这套工作流？

1. **告别低效手动录入**：单篇复制粘贴→批量抓取，10 分钟搞定数十篇公众号文章；
2. **突破平台功能局限**：相比腾讯 IMA 等工具，Dify 知识库可联动 AI 模型、工作流，直接赋能内容创作、客户咨询等业务；
3. **内容精准无失真**：爬虫抓取 + AI 提炼 + 格式标准化，确保存入知识库的内容完整、干净；
4. **零代码门槛**：全程可视化配置，懂基础电脑操作就能上手，技术小白也能轻松驾驭。



## 二、前期准备：3 步搞定基础配置

在搭建工作流前，需完成 3 项核心准备，每一步都有关键细节，务必留意！

注册一个cloud.dify.ai账号



### 1. 创建 Dify 知识库并获取 ID

登录 Dify 平台，进入「工作室→知识库」，点击「创建一个空知识库」（无需提前上传内容）；

![img](https://imgoss.xgss.net/picgo-tx2025/QQ_1766992685013.png?tx)

关键操作：创建后，从知识库 URL 中提取「知识库 ID」—— 格式为 `https://cloud.dify.ai/datasets/{知识库ID}/documents`，中间的字符串即为 ID（如 `5d6e30a3-08ec-44e2-8f79-a1baf6c2e5d3`），后续配置需反复用到。

![img](H:\typora_images\QQ_1766992773980.png)



### 2. 生成 Dify API 密钥（核心鉴权）

由于 Dify 暂未提供直接写入知识库的插件，需通过 HTTP 请求实现交互，API 密钥是关键凭证：

- 路径：Dify 平台→「工作室→知识库→服务API」，点击「新建 API 密钥」；
- 安全提醒：API 密钥可操作所有知识库，务必存储在后端，切勿泄露给他人或暴露在客户端；
- 记住格式：后续 HTTP 请求需在请求头中携带 `Authorization: Bearer {你的API密钥}`。

![img](https://imgoss.xgss.net/picgo-tx2025/QQ_1766992926918.png?tx)

点击API文档

通过文本创建文档，点击试一试！

![img](H:\typora_images\QQ_1766993207245.png)





### 3. 安装 Firecrawl 爬虫工具并获取 API

爬虫是实现网页内容抓取的核心，这里选择轻量高效的 Firecrawl：

第一步：在 Dify 插件市场搜索「Firecrawl」，点击安装并授权；

![img](https://imgoss.xgss.net/picgo-tx2025/QQ_1766994218125.png?tx)



第二步：访问 Firecrawl 官网（firecrawl.dev），注册账号后进入「API Keys」生成密钥（免费额度足够测试与小规模使用）；

注意事项：免费额度有限，批量抓取前可查看用量统计，避免超出配额。

![img](https://imgoss.xgss.net/picgo-tx2025/QQ_1766994525773.png?tx)

免费计划

![img](H:\typora_images\QQ_1766994439989.png)

A lightweight way to try scraping. No cost, no card, no hassle.： 轻量化爬虫试用方案，零成本、免绑卡、无额外操作负担。

scrape 500 pages, 2 concurrent requests ,low rate limits: 爬取 500 个页面，支持 2 个并发请求，速率限制较低。

把Firecrawl的API KEY填入

![img](https://imgoss.xgss.net/picgo-tx2025/QQ_1766995451532.png?tx)



## 三、分阶段实操：从单页抓取到批量导入



### 单页面抓取：搞定单篇公众号文章导入

核心逻辑：提取用户输入的公众号网址→Firecrawl 抓取页面内容→AI 提炼标题与正文→转义格式→HTTP 请求存入知识库。

新建一个工作流



#### 1. 工作流节点配置（全程可视化）

节点顺序：**开始→参数提取器→单页面抓取→提取标题与正文→转义特殊符号→HTTP 请求→直接回复**

#### 2. 关键节点详细设置

- **参数提取器**：
  - 模型选择「grok-3-beta CHAT」，输入变量绑定「开始 /sys.query」；
  - 提取参数设置为 `website: Array[String]`（网址数组），指令填写：「从输入内容中提取公众号文章网址，输出为数组格式」。
- **单页面抓取（核心避坑点）**：
  - 工具选择 Firecrawl 的「单页面抓取」；
  - 关键配置：
    - 输出格式：Markdown；
    - 仅抓取主要内容：True（过滤页头、页尾等无关信息）；
    - 请求头（重点！绕开反爬）：添加 `User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36`，伪装成浏览器访问；
    - 超时设置：30000ms（避免因网络问题导致抓取失败）。





- **提取标题与正文**：

  - 模型选择「gpt-4o CHAT」，输入变量绑定「单页面抓取 /output」；
  - 指令要求：「从输入的 Markdown 内容中提取标题（title）和正文（content），仅保留纯文本，过滤图片、表情等元素，确保内容完整无修改」；
  - 输出参数：`title: String`（标题）、`content: String`（正文）。

- **转义特殊符号（避坑关键）**：HTTP 请求对格式敏感，换行符、引号等会导致报错，需添加「代码执行」节点：

  javascript

  

  运行

  

  

  

  

  ```javascript
  function main({arg1}) {
    return {
      content: arg1.replace(/\\/g, '\\\\').replace(/"/g, '\\"').replace(/\n/g, '\\n').replace(/\r/g, '\\r').replace(/\t/g, '\\t')
    }
  }
  ```

  

  作用：统一转义特殊字符，确保内容格式符合 HTTP 请求要求。

- **HTTP 请求：存入知识库**：

  - 请求方式：POST，URL 填写 `https://api.dify.ai/v1/datasets/{知识库ID}/document/create-by-text`；

  - 鉴权配置：选择「Bearer Token」，填入 Dify API 密钥；

  - 请求头：必须添加 `Content-Type: application/json`（否则无法识别格式）；

  - 请求体（JSON 格式）：

    json

    

    

    

    

    

    ```json
    {
      "name": "{{提取标题与正文/title}}",
      "text": "{{转义特殊符号/content}}",
      "indexing_technique": "high_quality",
      "process_rule": {"mode": "automatic"}
    }
    ```

    

  - 小技巧：直接复制 Dify API 文档中的 cURL 示例，可自动填充请求格式，只需替换变量即可。

#### 3. 测试运行

输入任意公众号文章 URL（如 `https://mp.weixin.qq.com/s/qFrdb5xJCaVTOJIXvpwnBw`），运行工作流。若返回状态码 200，且在 Dify 知识库中能看到对应文章，则单页面抓取配置成功！



### 批量导入：一次性搞定 N 篇公众号文章

单篇抓取满足基础需求，批量导入才是效率核心。核心思路是通过「迭代」节点遍历 URL 数组，循环执行单页面抓取流程。

#### 1. 批量获取公众号 URL

推荐两种高效方式：

- 方式 1：使用浏览器插件（如「标签批量采集助手」），一键采集当前所有打开的公众号文章 URL，自动生成数组格式；
- 方式 2：手动整理 URL 列表，按 `["URL1", "URL2", ..., "URLn"]` 格式输入。

#### 2. 工作流升级：添加「迭代」节点

- 节点位置：「参数提取器」之后，「单页面抓取」之前；
- 核心配置：
  - 输入列表：绑定「参数提取器 /website」（必须是数组格式）；
  - 循环逻辑：将「单页面抓取→提取标题与正文→转义特殊符号→HTTP 请求」完整流程嵌套进迭代节点；
  - 并行模式：默认「错误时终止」，避免单篇失败影响整体流程。

#### 3. 关键注意事项

- 参数提取器需修改指令：「从输入内容中找出所有公众号网址，组成数组格式输出」；
- 迭代节点的输入必须是 Array 类型，否则无法触发循环；
- 批量抓取时建议控制 URL 数量（单次 10-20 篇），避免触发 Firecrawl 配额限制或公众号反爬机制。

测试后，知识库会批量新增所有文章，每篇均按「标题 + 纯文本正文」格式存储，字符数、状态清晰可见，效率直接拉满！

### （黄金阶段）进阶展望：自动爬虫，让知识库 “自给自足”

目前的青铜、白银方案已能满足大部分场景，而黄金阶段的核心是「自动化 + 智能化」，无需手动提供 URL：

1. **AI 深度搜索联动**：借助 Dify 的搜索能力，输入关键词（如「高中数学思维方法」），让 AI 自动搜索相关公众号文章，提取 URL 后触发批量抓取流程；
2. **Agent 自主浏览**：通过 Dify Agent 的浏览器交互能力，让 AI 自主识别目标公众号的相关文章，自动抓取并入库，实现 “主题驱动→内容抓取→知识库更新” 的全自动化闭环。

这套基础工作流可作为核心工具，后续只需对接搜索或 Agent 模块，即可实现知识库的 “自给自足”，后续我们会单独拆解这部分进阶内容，感兴趣的朋友可以持续关注！

## 四、总结：让知识库成为业务增长引擎

这套 Dify 爬虫工作流的核心优势在于「零代码、高适配、强落地」—— 无需编程基础，通过可视化节点配置，就能解决公众号文章批量入库的核心痛点。无论是教培行业搭建垂直学科知识库，还是自媒体整理创作素材，亦或是企业沉淀行业干货，都能快速落地。

关键记住 3 个核心避坑点：① 公众号反爬需配置 User-Agent 请求头；② 特殊符号必须转义避免 HTTP 请求报错；③ 批量抓取需用迭代节点且输入为数组格式。

按照教程操作，1 小时即可搭建完成属于自己的自动化知识库入库流程。后续我们还会分享「如何调用知识库赋能 AI 创作」「多平台内容（小红书、知乎）批量入库」等实操内容，欢迎加入 AI 交流群，一起解锁更多高效工具用法！