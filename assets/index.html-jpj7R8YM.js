import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as i,o as e}from"./app-BiQR_lPj.js";const l={};function p(d,s){return e(),n("div",null,s[0]||(s[0]=[i(`<h1 id="ubuntu系统下部署大语言模型-ollama和openwebui实现各大模型的人工智能自由" tabindex="-1"><a class="header-anchor" href="#ubuntu系统下部署大语言模型-ollama和openwebui实现各大模型的人工智能自由"><span>Ubuntu系统下部署大语言模型：Ollama和OpenWebUI实现各大模型的人工智能自由</span></a></h1><p>之前在window下安装过 Ollama和OpenWebUI搭建本地的人工智能web项目（可以看我之前写的文章），无奈电脑硬件配置太低，用qwen32b就很卡，卡出PPT了，于是又找了一台机器安装linux系统，在linux系统下测试一下速度能否可以快一些。</p><h1 id="系统硬件介绍" tabindex="-1"><a class="header-anchor" href="#系统硬件介绍"><span>系统硬件介绍</span></a></h1><p>Ubuntu 22.04.4 LTS</p><p>CPU: i5-10400F</p><p>内存：32G</p><p>硬盘： 512G SSD</p><p>显卡： NVIDIA GeForce GTX 1060 6GB</p><p>内网IP: 192.168.1.21</p><p><img src="https://imgoss.xgss.net/picgo/ubuntu-Ollama-OpenWebUI.png?aliyun" alt="ubuntu-Ollama-OpenWebUI"></p><h1 id="下载-ollama" tabindex="-1"><a class="header-anchor" href="#下载-ollama"><span>下载 Ollama</span></a></h1><p>访问下载： https://ollama.com/</p><p><img src="https://imgoss.xgss.net/picgo/image-20240517160214023.png?aliyun" alt="image-20240517160214023"></p><h2 id="安装ollama" tabindex="-1"><a class="header-anchor" href="#安装ollama"><span>安装Ollama</span></a></h2><h2 id="方法1、命令行下载安装-耗时长" tabindex="-1"><a class="header-anchor" href="#方法1、命令行下载安装-耗时长"><span>方法1、命令行下载安装(耗时长)</span></a></h2><p>安装命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo apt  install curl</span></span>
<span class="line"><span></span></span>
<span class="line"><span>$ curl -fsSL https://ollama.com/install.sh | sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240517160657340.png?aliyun" alt="image-20240517160657340"></p><p>缺点： 国内网络环境要等很久</p><h2 id="方法2-手动下载安装" tabindex="-1"><a class="header-anchor" href="#方法2-手动下载安装"><span>方法2 , 手动下载安装</span></a></h2><p>1、手动下载 https://ollama.com/install.sh 这个文件</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo mkdir ollama</span></span>
<span class="line"><span>cd ollama</span></span>
<span class="line"><span>$ sudo wget https://ollama.com/install.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、注释掉下载部分 curl xxxx 手动下载ollama-linux-{ARCH}</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo vim install.sh</span></span>
<span class="line"><span></span></span>
<span class="line"><span>修改文件：</span></span>
<span class="line"><span>status &quot;Downloading ollama...&quot;</span></span>
<span class="line"><span>#curl --fail --show-error --location --progress-bar -o $TEMP_DIR/ollama &quot;https://ollama.com/download/ollama-linux-\${ARCH}\${VER_PARAM}&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我电脑intel/amd cpu 所以 {ARCH} = amd64 浏览器下载 https://ollama.com/download/ollama-linux-amd64 当然科学上网速度更快哟。 放在 install.sh 同目录下</p><p>3、注释掉 #$SUDO install -o0 -g0 -m755 $TEMP_DIR/ollama $BINDIR/ollama</p><p>改为下面一行：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo vim install.sh</span></span>
<span class="line"><span></span></span>
<span class="line"><span>修改文件：</span></span>
<span class="line"><span>status &quot;Installing ollama to $BINDIR...&quot;</span></span>
<span class="line"><span>$SUDO install -o0 -g0 -m755 -d $BINDIR</span></span>
<span class="line"><span>#$SUDO install -o0 -g0 -m755 $TEMP_DIR/ollama $BINDIR/ollama</span></span>
<span class="line"><span>$SUDO install -o0 -g0 -m755 ./ollama-linux-amd64  $BINDIR/ollama</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>4 运行 install.sh ,安装</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sh  ./install.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240517171750382.png?aliyun" alt="image-20240517171750382"></p><p><img src="https://imgoss.xgss.net/picgo/image-20240517171944028.png?aliyun" alt="image-20240517171944028"></p><p>重启电脑</p><p>配置模型下载路径</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>cd </span></span>
<span class="line"><span>sudo vim .bashrc</span></span>
<span class="line"><span></span></span>
<span class="line"><span>sudo mkdir -p /home/star/ollama/ollama_cache</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后添加一行 配置 OLLAMA_MODELS 环境变量自定义路径</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>### ollama model dir 改为自己的路径</span></span>
<span class="line"><span># export OLLAMA_MODELS=/path/ollama_cache</span></span>
<span class="line"><span>export OLLAMA_MODELS=/home/star/ollama/ollama_cache</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果开始没配置OLLAMA_MODELS ，默认路径是/usr/share/ollama/.ollama/models</p><h2 id="启动ollama服务" tabindex="-1"><a class="header-anchor" href="#启动ollama服务"><span>启动ollama服务</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span># ollama --help</span></span>
<span class="line"><span>Large language model runner</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Usage:</span></span>
<span class="line"><span>  ollama [flags]</span></span>
<span class="line"><span>  ollama [command]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Available Commands:</span></span>
<span class="line"><span>  serve       Start ollama</span></span>
<span class="line"><span>  create      Create a model from a Modelfile</span></span>
<span class="line"><span>  show        Show information for a model</span></span>
<span class="line"><span>  run         Run a model</span></span>
<span class="line"><span>  pull        Pull a model from a registry</span></span>
<span class="line"><span>  push        Push a model to a registry</span></span>
<span class="line"><span>  list        List models</span></span>
<span class="line"><span>  ps          List running models</span></span>
<span class="line"><span>  cp          Copy a model</span></span>
<span class="line"><span>  rm          Remove a model</span></span>
<span class="line"><span>  help        Help about any command</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Flags:</span></span>
<span class="line"><span>  -h, --help      help for ollama</span></span>
<span class="line"><span>  -v, --version   Show version information</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Use &quot;ollama [command] --help&quot; for more information about a command.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>提示</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>star@star-ai:~$ ollama serve</span></span>
<span class="line"><span>Couldn&#39;t find &#39;/home/star/.ollama/id_ed25519&#39;. Generating new private key.</span></span>
<span class="line"><span>Your new public key is: </span></span>
<span class="line"><span></span></span>
<span class="line"><span>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPmYsSi6aIsyhC4EHEsCdBtSOqnfKmNVSf0Ofz9sVzyB</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Error: listen tcp 127.0.0.1:11434: bind: address already in use</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>说明已经运行</p><h2 id="修改ollama端口" tabindex="-1"><a class="header-anchor" href="#修改ollama端口"><span>修改ollama端口</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>vim /etc/systemd/system/ollama.service</span></span>
<span class="line"><span>在 [Service] 下添加  Environment=&quot;OLLAMA_HOST=0.0.0.0&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>cat /etc/systemd/system/ollama.service</span></span>
<span class="line"><span>[Unit]</span></span>
<span class="line"><span>Description=Ollama Service</span></span>
<span class="line"><span>After=network-online.target</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[Service]</span></span>
<span class="line"><span>ExecStart=/usr/local/bin/ollama serve</span></span>
<span class="line"><span>User=ollama</span></span>
<span class="line"><span>Group=ollama</span></span>
<span class="line"><span>Restart=always</span></span>
<span class="line"><span>RestartSec=3</span></span>
<span class="line"><span>Environment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin&quot;</span></span>
<span class="line"><span>Environment=&quot;OLLAMA_HOST=0.0.0.0&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[Install]</span></span>
<span class="line"><span>WantedBy=default.target</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>重新加载配置，重启ollama</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>systemctl daemon-reload</span></span>
<span class="line"><span></span></span>
<span class="line"><span>systemctl restart ollama</span></span>
<span class="line"><span></span></span>
<span class="line"><span>关闭服务</span></span>
<span class="line"><span>systemctl stop ollama</span></span>
<span class="line"><span>启动服务</span></span>
<span class="line"><span>systemctl start ollama</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="运行qwen大模型" tabindex="-1"><a class="header-anchor" href="#运行qwen大模型"><span>运行qwen大模型</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama run  qwen</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240517173411382.png?aliyun" alt="image-20240517173411382"></p><h1 id="安装docker" tabindex="-1"><a class="header-anchor" href="#安装docker"><span>安装docker</span></a></h1><p>一键安装脚本</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo curl -sSL https://get.docker.com/ | sh</span></span>
<span class="line"><span></span></span>
<span class="line"><span>安装完成之后</span></span>
<span class="line"><span>star@star-ai:~$ sudo docker --version</span></span>
<span class="line"><span>Docker version 26.1.3, build b72abbb</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="安装open-webui" tabindex="-1"><a class="header-anchor" href="#安装open-webui"><span>安装Open WebUI</span></a></h1><p>Open WebUI是一个用于在本地运行大型语言模型（LLM）的开源Web界面。</p><p>参考： https://docs.openwebui.com/getting-started/#quick-start-with-docker-</p><h3 id="docker安装open-webui" tabindex="-1"><a class="header-anchor" href="#docker安装open-webui"><span>docker安装open-webui</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>要运行支持 Nvidia GPU 的 Open WebUI，请使用以下命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo docker run -d \\</span></span>
<span class="line"><span>--name open-webui \\</span></span>
<span class="line"><span>--restart always \\</span></span>
<span class="line"><span> -p 3000:8080 \\</span></span>
<span class="line"><span>--gpus all --add-host=host.docker.internal:host-gateway \\</span></span>
<span class="line"><span>-v open-webui:/app/backend/data \\</span></span>
<span class="line"><span>ghcr.io/open-webui/open-webui:cuda</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>改国内的地址</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always  registry.cn-shenzhen.aliyuncs.com/funet8/open-webui:cuda</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>报错：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always  registry.cn-shenzhen.aliyuncs.com/funet8/open-webui:cuda</span></span>
<span class="line"><span>254b47e7994b2f0087ce0058918621523b39cf9b0e89018777c0cf98943ba2d1</span></span>
<span class="line"><span>docker: Error response from daemon: could not select device driver &quot;&quot; with capabilities: [[gpu]].</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>ubuntu识别不了我的显卡</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>$ sudo nvidia-smi</span></span>
<span class="line"><span>Fri May 17 18:37:15 2024       </span></span>
<span class="line"><span>+-----------------------------------------------------------------------------------------+</span></span>
<span class="line"><span>| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |</span></span>
<span class="line"><span>|-----------------------------------------+------------------------+----------------------+</span></span>
<span class="line"><span>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |</span></span>
<span class="line"><span>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |</span></span>
<span class="line"><span>|                                         |                        |               MIG M. |</span></span>
<span class="line"><span>|=========================================+========================+======================|</span></span>
<span class="line"><span>|   0  NVIDIA GeForce GTX 1060 6GB    Off |   00000000:01:00.0 Off |                  N/A |</span></span>
<span class="line"><span>| 40%   33C    P8              6W /  120W |      65MiB /   6144MiB |      0%      Default |</span></span>
<span class="line"><span>|                                         |                        |                  N/A |</span></span>
<span class="line"><span>+-----------------------------------------+------------------------+----------------------+</span></span>
<span class="line"><span>                                                                                         </span></span>
<span class="line"><span>+-----------------------------------------------------------------------------------------+</span></span>
<span class="line"><span>| Processes:                                                                              |</span></span>
<span class="line"><span>|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |</span></span>
<span class="line"><span>|        ID   ID                                                               Usage      |</span></span>
<span class="line"><span>|=========================================================================================|</span></span>
<span class="line"><span>|    0   N/A  N/A      1030      G   /usr/lib/xorg/Xorg                             56MiB |</span></span>
<span class="line"><span>|    0   N/A  N/A      1109      G   /usr/bin/gnome-shell                            4MiB |</span></span>
<span class="line"><span>+-----------------------------------------------------------------------------------------+</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>安装nvidia-container-toolkit： 确保你已经安装了nvidia-container-toolkit，并配置Docker以使用该工具包：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo apt-get update</span></span>
<span class="line"><span>sudo apt-get install -y nvidia-container-toolkit</span></span>
<span class="line"><span>sudo systemctl restart docker</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>检查Docker默认运行时配置： 确保Docker的默认运行时设置为nvidia。编辑Docker的配置文件（通常位于/etc/docker/daemon.json），并添加或修改如下内容：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo vim /etc/docker/daemon.json</span></span>
<span class="line"><span>添加：</span></span>
<span class="line"><span></span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>  &quot;default-runtime&quot;: &quot;nvidia&quot;,</span></span>
<span class="line"><span>  &quot;runtimes&quot;: {</span></span>
<span class="line"><span>    &quot;nvidia&quot;: {</span></span>
<span class="line"><span>      &quot;path&quot;: &quot;nvidia-container-runtime&quot;,</span></span>
<span class="line"><span>      &quot;runtimeArgs&quot;: []</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>编辑完文件后，重启Docker服务：</span></span>
<span class="line"><span>sudo systemctl restart docker</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>检查NVIDIA Container Runtime兼容性： 确保你的NVIDIA Container Runtime版本与Docker版本兼容。可以通过以下命令查看版本：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo docker version</span></span>
<span class="line"><span></span></span>
<span class="line"><span>nvidia-container-runtime --version</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>完成上述步骤后，再次尝试运行你的Docker命令。如果问题仍然存在，请提供更多的系统信息和日志，以便进一步诊断问题。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>sudo docker start open-webui</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240517184705558.png?aliyun" alt="image-20240517184705558"></p><h1 id="登录open-webui" tabindex="-1"><a class="header-anchor" href="#登录open-webui"><span>登录open-webui</span></a></h1><p>用IP+端口访问</p><p><img src="https://imgoss.xgss.net/picgo/image-20240517184836945.png?aliyun" alt="image-20240517184836945"></p><h2 id="修改语言为中文" tabindex="-1"><a class="header-anchor" href="#修改语言为中文"><span>修改语言为中文</span></a></h2><p>OpenWebUI默认是英文的，所以修改语言为简体中文。</p><p><img src="https://imgoss.xgss.net/picgo/image-20240518130431610.png?aliyun" alt="image-20240518130431610"></p><h2 id="openwebui不能连接ollama" tabindex="-1"><a class="header-anchor" href="#openwebui不能连接ollama"><span>OpenWebUI不能连接Ollama</span></a></h2><p>报错：WebUI could not connect to ollama</p><p><img src="https://imgoss.xgss.net/picgo/image-20240518130617215.png?aliyun" alt="image-20240518130617215"></p><p>修改地址：http://192.168.1.21:11434</p><p><img src="https://imgoss.xgss.net/picgo/image-20240518163725720.png?aliyun" alt="image-20240518163725720"></p><p>再下载千问的模型 qwen</p><p><img src="https://imgoss.xgss.net/picgo/image-20240518164005249.png?aliyun" alt="image-20240518164005249"></p><h1 id="下载大模型" tabindex="-1"><a class="header-anchor" href="#下载大模型"><span>下载大模型</span></a></h1><p>ollama官方的模型仓库参见这里：https://ollama.com/library</p><p><img src="https://imgoss.xgss.net/picgo/image-20240518165100596.png?aliyun" alt="image-20240518165100596"></p><p>根据自己的CPU和GPU选择合适的大模型，否则会很卡。</p><p>比如测试用的1060使用qwen:72b就很卡，问一个问题要等很久，几乎是不能用的状态。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>阿里巴巴的大模型：</span></span>
<span class="line"><span>ollama run  qwen</span></span>
<span class="line"><span>ollama run qwen:14b</span></span>
<span class="line"><span>ollama run qwen:32b</span></span>
<span class="line"><span>ollama run qwen:72b</span></span>
<span class="line"><span>ollama run qwen:110b   # 110b 表示该模型包含了 1100 亿（110 billion）个参数</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>脸书大模型：</span></span>
<span class="line"><span>ollama run llama2</span></span>
<span class="line"><span>ollama run llama3</span></span>
<span class="line"><span>ollama run llama3:8b</span></span>
<span class="line"><span></span></span>
<span class="line"><span>谷歌的大模型：</span></span>
<span class="line"><span>ollama run gemma</span></span>
<span class="line"><span></span></span>
<span class="line"><span>微软的大模型</span></span>
<span class="line"><span>ollama run phi3</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="删除模型" tabindex="-1"><a class="header-anchor" href="#删除模型"><span>删除模型</span></a></h1><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>显示所有模型</span></span>
<span class="line"><span># ollama list</span></span>
<span class="line"><span></span></span>
<span class="line"><span>删除模型</span></span>
<span class="line"><span># ollama rm llama3:latest</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240518162719946.png?aliyun" alt="image-20240518162719946"></p><h1 id="ubuntu查看gpu负载" tabindex="-1"><a class="header-anchor" href="#ubuntu查看gpu负载"><span>ubuntu查看GPU负载</span></a></h1><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>nvidia-smi</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>确实在ubuntu20.04系统下确实比window10系统使用Ollama更加流畅。</p>`,100)]))}const t=a(l,[["render",p]]),o=JSON.parse('{"path":"/article/y9wuiwqq/","title":"16.Ubuntu系统下部署大语言模型-Ollama和OpenWebUI实现各大模型的人工智能自由","lang":"en-US","frontmatter":{"title":"16.Ubuntu系统下部署大语言模型-Ollama和OpenWebUI实现各大模型的人工智能自由","createTime":"2025/05/27 17:51:17","permalink":"/article/y9wuiwqq/"},"git":{"createdTime":1749111496000,"updatedTime":1750129445000,"contributors":[{"name":"star","username":"star","email":"star@xgss.net","commits":2,"url":"https://github.com/star"}]},"readingTime":{"minutes":5.22,"words":1565},"filePathRelative":"chatgpt/16.Ubuntu系统下部署大语言模型-Ollama和OpenWebUI实现各大模型的人工智能自由.md"}');export{t as comp,o as data};
