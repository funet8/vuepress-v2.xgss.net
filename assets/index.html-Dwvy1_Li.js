import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as e,o as i}from"./app-BiQR_lPj.js";const l={};function p(t,a){return i(),n("div",null,a[0]||(a[0]=[e(`<h1 id="ollama模型导入指南-从huggingface下载模型轻松上手" tabindex="-1"><a class="header-anchor" href="#ollama模型导入指南-从huggingface下载模型轻松上手"><span>Ollama模型导入指南，从HuggingFace下载模型轻松上手</span></a></h1><p>大家好，我是星哥，上一篇文章星哥介绍了本地部署DeepSeek的方法：《<a href="https://mp.weixin.qq.com/s/kJ7JCgFUNKWtPtp8r5mR_A" target="_blank" rel="noopener noreferrer">简单3步部署本地国产DeepSeek大模型</a>》。</p><p>今天来讲不从<a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama官网</a>下载模型的方法，而是从HuggingFace下载，再导入模型。</p><p><img src="https://imgoss.xgss.net/picgo/image-20250207204841747.png?aliyun" alt="image-20250207204841747"></p><h1 id="一、安装ollama" tabindex="-1"><a class="header-anchor" href="#一、安装ollama"><span>一、安装ollama</span></a></h1><p>参照 https://mp.weixin.qq.com/s/kJ7JCgFUNKWtPtp8r5mR_A 这篇文章安装ollama</p><h2 id="_1-查看ollama版本" tabindex="-1"><a class="header-anchor" href="#_1-查看ollama版本"><span>1.查看ollama版本</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama -v</span></span>
<span class="line"><span>ollama version is 0.5.7</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_2-下载模型" tabindex="-1"><a class="header-anchor" href="#_2-下载模型"><span>2.下载模型</span></a></h2><blockquote><p>由于文件比较大，下载模型的时间花费比较长，</p><p>关注&#39;星哥说事&#39;，回复 &#39;HuggingFace-llama3&#39; 获得网盘下载地址。</p></blockquote><p>Ollama可以直接下载内置的几种模型，但选择有限。我们更希望从<a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">HuggingFace</a>下载以便方便地评估各种模型，所以，这里我们并不从Ollama直接下载，而是从HuggingFace下载。</p><p>在HuggingFace搜索<code>llama3</code>，设置<code>Languages</code>为<code>Chinese</code>，可以看到若干基于LLaMa3的中文模型：</p><p>直达地址： https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF/tree/main</p><p><img src="https://imgoss.xgss.net/picgo/image-20250207181753463.png?aliyun" alt="image-20250207181753463"></p><p>点击 Files and versions</p><p><img src="https://imgoss.xgss.net/picgo/image-20250207181816429.png?aliyun" alt="image-20250207181816429"></p><p>下载 Llama3-8B-Chinese-Chat.q6_k.GGUF</p><p>GGUF格式的模型，GGUF格式是llama.cpp团队搞的一种模型存储格式，一个模型就是一个文件，llama.cpp的创始人Georgi Gerganov定义，旨在解决当前大模型在实际应用中遇到的存储效率、加载速度、兼容性和扩展性等问题。</p><p><img src="https://imgoss.xgss.net/picgo/image-20250207181848645.png?aliyun" alt="image-20250207181848645"></p><h2 id="_3-导入模型" tabindex="-1"><a class="header-anchor" href="#_3-导入模型"><span>3.导入模型</span></a></h2><p>需要编写一个配置文件，随便起个名字，如ollama_Liama3_config.txt</p><p>文件放到D盘下的ollama目录中</p><p>配置文件内容如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>FROM &quot;D:\\ollama\\Llama3-8B-Chinese-Chat.q6_k.GGUF&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>TEMPLATE &quot;&quot;&quot;{{- if .System }}</span></span>
<span class="line"><span>&lt;|im_start|&gt;system {{ .System }}&lt;|im_end|&gt;</span></span>
<span class="line"><span>{{- end }}</span></span>
<span class="line"><span>&lt;|im_start|&gt;user</span></span>
<span class="line"><span>{{ .Prompt }}&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;assistant</span></span>
<span class="line"><span>&quot;&quot;&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>SYSTEM &quot;&quot;&quot;&quot;&quot;&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PARAMETER stop &lt;|im_start|&gt;</span></span>
<span class="line"><span>PARAMETER stop &lt;|im_end|&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20250207183237880.png?aliyun" alt="image-20250207183237880"></p><p>导入模型命令</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>PS D:\\ollama&gt; ollama create llama3-cn-8b -f ./ollama_Liama3_config.txt</span></span>
<span class="line"><span>gathering model components</span></span>
<span class="line"><span>copying file sha256:e0e83a7967c61e38d6a3fd8b093754117944b405d35afe95f129fbfb143929f2 100%</span></span>
<span class="line"><span>parsing GGUF</span></span>
<span class="line"><span>using existing layer sha256:e0e83a7967c61e38d6a3fd8b093754117944b405d35afe95f129fbfb143929f2</span></span>
<span class="line"><span>creating new layer sha256:b65f5bb03e74da8572e4191596a895bddc10355595c38574a16fdf12a889855b</span></span>
<span class="line"><span>creating new layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216</span></span>
<span class="line"><span>writing manifest</span></span>
<span class="line"><span>success</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>查看模型</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama list</span></span>
<span class="line"><span>NAME                     ID              SIZE      MODIFIED</span></span>
<span class="line"><span>llama3-cn-8b:latest      d710bb08d58c    6.6 GB    About a minute ago</span></span>
<span class="line"><span>llama2-chinese:latest    cee11d703eee    3.8 GB    19 hours ago</span></span>
<span class="line"><span>deepseek-r1:14b          ea35dfe18182    9.0 GB    27 hours ago</span></span>
<span class="line"><span>qwen2.5:latest           845dbda0ea48    4.7 GB    40 hours ago</span></span>
<span class="line"><span>deepseek-r1:7b           0a8c26691023    4.7 GB    42 hours ago</span></span>
<span class="line"><span>llama3:latest            365c0bd3c000    4.7 GB    45 hours ago</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>llama3-cn-8b:latest 则导入成功</p><h2 id="运行模型" tabindex="-1"><a class="header-anchor" href="#运行模型"><span>运行模型</span></a></h2><p>使用Ollama的<code>run</code>命令可以直接运行模型。我们输入命令</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama run llama3-cn-8b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20250207183706904.png?aliyun" alt="image-20250207183706904"></p><h1 id="在open-webui使用" tabindex="-1"><a class="header-anchor" href="#在open-webui使用"><span>在Open WebUI使用</span></a></h1><p><img src="https://imgoss.xgss.net/picgo/image-20250207183938856.png?aliyun" alt="image-20250207183938856"></p><h1 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h1><p>通过 Ollama，你可以轻松地在本地运行 Hugging Face 模型。Ollama 提供了简单易用的命令行界面，让你能够快速上手。希望本指南能够帮助你成功导入并运行 Hugging Face 模型。</p><p>写文不易，如果你都看到了这里，请点个赞和在看，分享给更多的朋友；也别忘了关注星哥玩云！这里有满满的干货分享，还有轻松有趣的技术交流～点个赞、分享给身边的小伙伴，一起成长，一起玩转技术世界吧！ 😊</p>`,39)]))}const d=s(l,[["render",p]]),g=JSON.parse('{"path":"/article/zi0cwlf3/","title":"27.Ollama模型导入指南，从HuggingFace下载模型轻松上手","lang":"en-US","frontmatter":{"title":"27.Ollama模型导入指南，从HuggingFace下载模型轻松上手","createTime":"2025/05/27 17:51:17","permalink":"/article/zi0cwlf3/"},"git":{"createdTime":1749111496000,"updatedTime":1750129445000,"contributors":[{"name":"star","username":"star","email":"star@xgss.net","commits":2,"url":"https://github.com/star"}]},"readingTime":{"minutes":2.59,"words":777},"filePathRelative":"chatgpt/27.Ollama模型导入指南，从HuggingFace下载模型轻松上手.md"}');export{d as comp,g as data};
