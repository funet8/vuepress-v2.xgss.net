import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,a as i,o as l}from"./app-BiQR_lPj.js";const t={};function n(p,e){return l(),s("div",null,e[0]||(e[0]=[i(`<h1 id="简单3步部署本地国产deepseek大模型" tabindex="-1"><a class="header-anchor" href="#简单3步部署本地国产deepseek大模型"><span>简单3步部署本地国产DeepSeek大模型</span></a></h1><p>DeepSeek是最近非常火的开源大模型，国产大模型 DeepSeek 凭借其优异的性能和对硬件资源的友好性，受到了众多开发者的关注。</p><p>无奈，在使用时候deepseek总是提示服务器繁忙，请稍后再试。</p><p><img src="https://imgoss.xgss.net/picgo/image-20250206154110517.png?aliyun" alt="image-20250206154110517"></p><p>本文将介绍如何通过简单 3 步在本地部署 DeepSeek 大模型，让你能够轻松体验这一强大的 AI 工具。</p><p><img src="https://imgoss.xgss.net/picgo/image-20250206160026953.png?aliyun" alt="image-20250206160026953"></p><h1 id="deepseek对世界的影响" tabindex="-1"><a class="header-anchor" href="#deepseek对世界的影响"><span>Deepseek对世界的影响</span></a></h1><h2 id="改变ai行业格局" tabindex="-1"><a class="header-anchor" href="#改变ai行业格局"><span>改变AI行业格局</span></a></h2><ul><li><p>冲击国际AI竞争态势：打破了美国等西方国家的AI巨头长期领先的局面，使全球AI竞争进入技术多极化时代，如微软、亚马逊云科技、英伟达等国际科技巨头纷纷宣布接入DeepSeek。</p></li><li><p>引发行业洗牌：其他AI公司感受到压力，加快技术研发和创新步伐，如OpenAI紧急上线新一代推理模型，阿里云发布升级版通义千问旗舰模型。</p></li></ul><h2 id="影响全球科技股市场" tabindex="-1"><a class="header-anchor" href="#影响全球科技股市场"><span>影响全球科技股市场</span></a></h2><ul><li><p>引发股价波动：英伟达等芯片制造商以及美国的微软、Meta和谷歌母公司Alphabet等科技巨头的股价受到冲击。</p></li><li><p>改变投资风向：吸引了大量资本的关注，让投资者对中国的AI产业有了更高的期待，可能会导致更多的资金流向中国的AI企业。</p></li></ul><h2 id="推动全球ai技术发展" tabindex="-1"><a class="header-anchor" href="#推动全球ai技术发展"><span>推动全球AI技术发展</span></a></h2><ul><li><p>提供技术思路：在模型架构、算法等方面实现了多项创新，如采用创新性的混合专家架构等，为全球AI研究人员提供了新的思路。</p></li><li><p>加速技术普及：开源策略让更多的开发者和企业能够接触到先进的AI技术，降低了AI技术的使用门槛，加速其在各个领域的普及。</p></li></ul><h2 id="影响社会文化领域" tabindex="-1"><a class="header-anchor" href="#影响社会文化领域"><span>影响社会文化领域</span></a></h2><ul><li><p>改变工作生活方式：在自然语言处理等方面的能力，可提高翻译、写作、代码生成等工作效率，在制定旅行攻略、翻译外语等日常生活场景中也广泛应用。</p></li><li><p>引发伦理监管讨论：国际社会对AI伦理问题关注度提升，数据安全、隐私保护和内容监管等问题成为焦点，促使各国制定更严格的AI大模型监管政策。</p></li></ul><h1 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备"><span>环境准备</span></a></h1><p>部署方案：<strong>Ollama</strong> + <strong>DeepSeek-R1</strong> + <strong>Open WebUI</strong></p><p>笔者的电脑硬件配置如下：</p><p>系统： Window11</p><p>CPU: 13th i7-13700KF</p><p>内存： 32G</p><p>显卡：Nvidia GeForce RTX 4070Ti</p><p>可以运行大模型deepseek-r1的哪个版本的大模型？</p><p><img src="https://imgoss.xgss.net/picgo/image-20250206173953157.png?aliyun" alt="image-20250206173953157"></p><p>问deepseek得到的回答：</p><p>最佳选择：优先尝试 DeepSeek-R1-7B 4-bit量化版，平衡速度和性能；若需要更高精度，可测试 DeepSeek-R1-13B 4-bit量化版（需确保显存无其他占用）。建议关注官方发布的轻量化版本或社区优化方案（如GPTQ）。</p><h1 id="运行deepseek-r1参数模型电脑配置要求" tabindex="-1"><a class="header-anchor" href="#运行deepseek-r1参数模型电脑配置要求"><span>运行DeepSeek-R1参数模型电脑配置要求</span></a></h1><p>大家可以参考下面的表格，给出的电脑配置要求。避免因为配置问题运行不了。</p><table><thead><tr><th style="text-align:left;"><strong>模型名称</strong></th><th style="text-align:left;"><strong>基础模型</strong></th><th style="text-align:left;"><strong>参数规模</strong></th><th style="text-align:left;"><strong>特点</strong></th><th style="text-align:left;"><strong>计算需求</strong></th></tr></thead><tbody><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Qwen-1.5B</strong></td><td style="text-align:left;">Qwen</td><td style="text-align:left;"><strong>1.5B</strong></td><td style="text-align:left;">轻量级，适合移动端和本地推理</td><td style="text-align:left;">低（PC 可运行）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Qwen-7B</strong></td><td style="text-align:left;">Qwen</td><td style="text-align:left;"><strong>7B</strong></td><td style="text-align:left;">适合本地推理和轻量级服务器部署</td><td style="text-align:left;">中等（24GB+ VRAM）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Llama-8B</strong></td><td style="text-align:left;">LLaMA</td><td style="text-align:left;"><strong>8B</strong></td><td style="text-align:left;">LLaMA 版本，英文能力更强</td><td style="text-align:left;">中等</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Qwen-14B</strong></td><td style="text-align:left;">Qwen</td><td style="text-align:left;"><strong>14B</strong></td><td style="text-align:left;">中文能力更强，适用于专业应用</td><td style="text-align:left;">较高（A100 级别 GPU）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Qwen-32B</strong></td><td style="text-align:left;">Qwen</td><td style="text-align:left;"><strong>32B</strong></td><td style="text-align:left;">适用于企业级 AI 任务</td><td style="text-align:left;">高（多 GPU / 云端）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-Distill-Llama-70B</strong></td><td style="text-align:left;">LLaMA</td><td style="text-align:left;"><strong>70B</strong></td><td style="text-align:left;">强大推理能力，适合高端 AI 计算</td><td style="text-align:left;"><strong>极高</strong>（H100 / TPU 级别）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-V3-671B</strong></td><td style="text-align:left;">自研</td><td style="text-align:left;"><strong>671B</strong></td><td style="text-align:left;"><strong>DeepSeek 最高参数量模型，超大规模通用 AI</strong></td><td style="text-align:left;"><strong>超高</strong>（云端 A100 集群）</td></tr><tr><td style="text-align:left;"><strong>DeepSeek-R1-671B</strong></td><td style="text-align:left;">自研</td><td style="text-align:left;"><strong>671B</strong></td><td style="text-align:left;"><strong>DeepSeek-R1 的完整超大模型</strong></td><td style="text-align:left;"><strong>超高</strong>（企业级/科研级）</td></tr></tbody></table><ol><li><strong>Qwen 系列（通义千问）</strong>：基于阿里巴巴的 Qwen（通义千问）系列，适合中文任务，拥有较强的知识推理能力和上下文理解能力。</li><li><strong>LLaMA 系列</strong>：基于 Meta 的 LLaMA（LLaMA 2/3），更加通用，适合多语言任务，在英文任务上表现较好。</li><li><strong>DeepSeek 自研模型（R1/V3）</strong>：DeepSeek 自主训练的大规模 Transformer 模型，R1 主要用于推理，而 V3 可能结合更多创新结构。</li></ol><h1 id="什么是ollama" tabindex="-1"><a class="header-anchor" href="#什么是ollama"><span>什么是Ollama</span></a></h1><p>Ollama是一个开源项目，旨在让用户能够轻松地在其本地计算机上运行大型语言模型（LLM），是一个开源的大型语言模型服务。它支持各种LLM，包括Llama 3、Mistral和Gemma。</p><p>提供了类似OpenAI的API接口和聊天界面,可以非常方便地部署最新版本的GPT模型并通过接口使用。支持热加载模型文件,无需重新启动即可切换不同的模型。</p><p><img src="https://imgoss.xgss.net/picgo/ollama.png?aliyun" alt="img"></p><p>Ollama官网： https://ollama.com/</p><p>Ollama GitHub仓库：https://github.com/ollama/ollama</p><h1 id="第一步、安装ollama" tabindex="-1"><a class="header-anchor" href="#第一步、安装ollama"><span>第一步、安装Ollama</span></a></h1><h2 id="下载ollama" tabindex="-1"><a class="header-anchor" href="#下载ollama"><span>下载Ollama</span></a></h2><p>Ollama下载地址：https://ollama.com/download</p><p>支持macOS、Linux、Windows系统，根据自己的系统，下载安装包：</p><p>Download for windows</p><p><img src="https://imgoss.xgss.net/picgo/image-20250206165950047.png?aliyun" alt="image-20250206165950047"></p><h2 id="安装ollama" tabindex="-1"><a class="header-anchor" href="#安装ollama"><span>安装Ollama</span></a></h2><p>windows系统下安装也比较方便，双击打开 install</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511151331009.png?aliyun" alt="image-20240511151331009"></p><p>安装完成没有提示，我们打开一个终端，本文以Windows PowerShell为例，大家也可以使用其他的：</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511151503523.png?aliyun" alt="image-20240511151503523"></p><p>现在Ollama已经安装完了，我们需要在终端中输入下方命令运行一个大语言模型进行测试，这里以对在中文方面表现相对好些的千问为例，大家也可以使用其他的模型。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama run qwen</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240511151819825.png?aliyun" alt="image-20240511151819825"></p><p>安装成功，随便问几个问题。</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511151923267.png?aliyun" alt="image-20240511151923267"></p><p>理论上就安装完成了，可以只在命令行中使用大模型了。</p><h2 id="修改路径文件保存路径" tabindex="-1"><a class="header-anchor" href="#修改路径文件保存路径"><span>修改路径文件保存路径</span></a></h2><p>可以不用改，如果C盘空间不够用，建议修改。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>可以看到，系统正在下载qwen的模型（并保存在C盘，C:\\Users&lt;username&gt;.ollama\\models 如果想更改默认路径，可以通过设置OLLAMA_MODELS进行修改，然后重启终端，重启ollama服务。）</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>setx OLLAMA_MODELS &quot;D:\\ollama\\model&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="运行deepseek" tabindex="-1"><a class="header-anchor" href="#运行deepseek"><span>运行DeepSeek</span></a></h2><p>https://ollama.com/library/deepseek-r1</p><p>DeepSeek-R1模型有多个版本，可以根据需要选择不同版本，例如 ollama run deepseek-r1:671b，详情如下（模型参数越大，需要配置越高）：</p><p>在DeepSeek-R1系列中，还有1.5B、7B、8B、14B、32B、70B、671B等不同参数规模的型号。这些不同规模的模型在模型能力、资源需求和应用场景上有所不同。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>ollama run deepseek-r1:1.5b  【电脑配置低的可以运行此版本，根据文件大小和网络情况，下载时间也不确定】</span></span>
<span class="line"><span>ollama run deepseek-r1:7b</span></span>
<span class="line"><span>ollama run deepseek-r1:8b</span></span>
<span class="line"><span>ollama run deepseek-r1:14b</span></span>
<span class="line"><span>ollama run deepseek-r1:32b</span></span>
<span class="line"><span>ollama run deepseek-r1:70b</span></span>
<span class="line"><span>ollama run deepseek-r1:671b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我这里选择运行7b（70亿参数）</p><p><img src="https://imgoss.xgss.net/picgo/VQIWCEW7DF2SBNLL76S(8)S.png?aliyun" alt="img"></p><h2 id="ollama常用命令-可忽略" tabindex="-1"><a class="header-anchor" href="#ollama常用命令-可忽略"><span>Ollama常用命令（可忽略）</span></a></h2><p>省流版命令： <code>ollama list</code> 和 ollama rm [模型名称] 。</p><h3 id="模型管理" tabindex="-1"><a class="header-anchor" href="#模型管理"><span>模型管理</span></a></h3><ol><li><strong>创建模型</strong><ul><li>命令：<code>ollama create [Modelfile路径]</code></li><li>功能：使用包含模型信息的Modelfile来创建一个新模型。</li></ul></li><li><strong>显示模型信息</strong><ul><li>命令1：<code>ollama show [模型名称]</code></li><li>功能：显示特定模型的详细信息，如模型名称、版本等。</li><li>命令2：<code>/show</code>（在会话界面中使用）</li></ul></li><li><strong>列出所有模型</strong><ul><li>命令1：<code>ollama list</code></li><li>命令2：<code>ollama ls</code></li><li>功能：列出本地所有可用的模型，可以在这里查找模型名称。</li></ul></li><li><strong>从注册表中拉取模型</strong><ul><li>命令：<code>ollama pull [模型名称]</code></li><li>功能：从模型注册表中拉取一个模型到本地使用。</li></ul></li><li><strong>将模型推送到注册表</strong><ul><li>命令：<code>ollama push [模型名称]</code></li><li>功能：将本地模型推送到模型注册表中，以便他人或其他系统使用。</li></ul></li><li><strong>复制模型</strong><ul><li>命令：<code>ollama cp [原模型名称] [新模型名称]</code></li><li>功能：复制一个模型到另一个位置或给定名称的地方。</li></ul></li><li><strong>删除模型</strong><ul><li>命令：<code>ollama rm [模型名称]</code></li><li>功能：删除一个已安装的模型。</li></ul></li></ol><h3 id="模型运行与会话管理" tabindex="-1"><a class="header-anchor" href="#模型运行与会话管理"><span>模型运行与会话管理</span></a></h3><ol><li><strong>运行模型</strong><ul><li>命令：<code>ollama run [模型名称]</code></li><li>功能：运行一个已安装的模型，执行某些任务。可以根据需要指定模型的参数和配置。</li></ul></li><li><strong>加载模型</strong><ul><li>命令：<code>/load &lt;model&gt;</code></li><li>功能：在会话界面中加载一个特定的模型或会话。可以指定一个模型的名称或路径来加载它。</li></ul></li><li><strong>保存模型或会话状态</strong><ul><li>命令：<code>/save &lt;model&gt;</code></li><li>功能：在会话界面中保存当前的会话状态或模型。可以将当前会话或模型的配置保存为一个文件，以便以后使用。</li></ul></li><li><strong>清理上下文</strong><ul><li>命令：<code>/clear</code></li><li>功能：清除会话上下文。这将删除当前会话中的所有历史记录或对话内容。</li></ul></li><li><strong>退出对话模型</strong><ul><li>命令：<code>/bye</code></li><li>功能：退出当前与模型的对话，并退出程序。</li></ul></li></ol><h3 id="其他命令" tabindex="-1"><a class="header-anchor" href="#其他命令"><span>其他命令</span></a></h3><ol><li><strong>获取帮助信息</strong><ul><li>命令1：<code>ollama help [命令名称]</code></li><li>命令2：<code>ollama --help</code></li><li>功能：获取有关Ollama任何命令的帮助信息。如果指定了命令名称，则显示该命令的详细帮助信息。</li></ul></li><li><strong>查看版本信息</strong><ul><li>命令1：<code>ollama version</code></li><li>命令2：<code>ollama -v</code></li><li>命令3：<code>ollama --version</code></li><li>功能：显示当前Ollama工具的版本信息。</li></ul></li><li><strong>查看快捷键</strong><ul><li>命令：<code>/?shortcuts</code></li><li>功能：在会话界面中查看键盘快捷键的帮助信息，帮助更快速地进行操作。</li></ul></li><li><strong>设置会话参数和配置</strong><ul><li>命令：<code>/set 参数名 参数值</code></li><li>功能：用于设置会话参数和配置，例如设置消息格式、启用或禁用历史记录等。</li></ul></li></ol><h1 id="第二步、安装docker" tabindex="-1"><a class="header-anchor" href="#第二步、安装docker"><span>第二步、安装Docker</span></a></h1><p>Open WebUI是一个用于在本地运行大型语言模型（LLM）的开源Web界面。</p><p>Open WebUI是在docker中安装的，所以要先安装docker。</p><h2 id="在window下安装docker" tabindex="-1"><a class="header-anchor" href="#在window下安装docker"><span>在window下安装docker</span></a></h2><h3 id="_1-启动hyper-v" tabindex="-1"><a class="header-anchor" href="#_1-启动hyper-v"><span>1.启动Hyper-v</span></a></h3><p>打开控制面板，在程序与功能页面选择启用或Windows功能</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511153112805.png?aliyun" alt="image-20240511153112805"></p><p>然后，重启计算机。</p><h3 id="_2-安装wsl" tabindex="-1"><a class="header-anchor" href="#_2-安装wsl"><span>2.安装WSL</span></a></h3><p>打开 powershell，以管理员的身份启动命令窗口，输入</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>wsl --install</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果不是已管理员身份启动则会报错：请求的操作需要提升</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511154319596.png?aliyun" alt="image-20240511154319596"></p><p>然后再次重启电脑。</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511172942703.png?aliyun" alt="image-20240511172942703"></p><h3 id="_3-下载docker软件" tabindex="-1"><a class="header-anchor" href="#_3-下载docker软件"><span>3.下载Docker软件</span></a></h3><p>点击下载链接：https://docs.docker.com/desktop/install/windows-install/</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511153811506.png?aliyun" alt="image-20240511153811506"></p><h3 id="_4-安装docker" tabindex="-1"><a class="header-anchor" href="#_4-安装docker"><span>4.安装Docker</span></a></h3><p><img src="https://imgoss.xgss.net/picgo/image-20240511155742842.png?aliyun" alt="image-20240511155742842"></p><h1 id="第三步、使用docker部署open-webui" tabindex="-1"><a class="header-anchor" href="#第三步、使用docker部署open-webui"><span>第三步、使用Docker部署Open WebUI</span></a></h1><p>开源地址： https://github.com/open-webui/open-webui</p><p>参考： https://github.com/open-webui/open-webui?tab=readme-ov-file#installation-with-default-configuration</p><p>可以看到，如果你的Ollama和Open WebUI在同一台主机，那使用下面显示的这一行命令就可以在本地快速进行部署：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</span></span>
<span class="line"><span></span></span>
<span class="line"><span>由于我的电脑有GPU</span></span>
<span class="line"><span></span></span>
<span class="line"><span>此时我使用的是：</span></span>
<span class="line"><span>docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>现在我们打开终端，比如powershell，然后输入docker，回车</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511173724816.png?aliyun" alt="image-20240511173724816"></p><p>然后将上边在docker中部署Open WebUI的命令复制后粘贴到终端中，回车。</p><h2 id="docker安装open-webui" tabindex="-1"><a class="header-anchor" href="#docker安装open-webui"><span>docker安装open-webui</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>要运行支持 Nvidia GPU 的 Open WebUI，请使用以下命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果您仅使用 OpenAI API，请使用以下命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果仅适用于 CPU，不使用 GPU，请改用以下命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><img src="https://imgoss.xgss.net/picgo/image-20240511173828471.png?aliyun" alt="image-20240511173828471"></p><h2 id="使用浏览器open-webui" tabindex="-1"><a class="header-anchor" href="#使用浏览器open-webui"><span>使用浏览器Open WebUI</span></a></h2><p>安装完成后，在Docker Desktop中可以看到Open WebUI的web界面地址为：http://localhost:3000</p><p>或者内网IP+端口，这样局域网的其他人也可以访问到，甚至是通过内网穿透，让其他人也可以访问。</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511175350983.png?aliyun" alt="image-20240511175350983"></p><p>注册账号点击 sign up</p><p>登录</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511175608965.png?aliyun" alt="image-20240511175608965"></p><h2 id="设置为简体中文" tabindex="-1"><a class="header-anchor" href="#设置为简体中文"><span>设置为简体中文</span></a></h2><p>点击右上角的设置，可以修改当前界面的语言为简体中文：然后点击保存即可。</p><p><img src="https://imgoss.xgss.net/picgo/image-20240511175737460.png?aliyun" alt="image-20240511175737460"></p><p>点击上方选择一个模型旁边的加号+可以增加大模型，点击下拉按钮可以选择当前使用哪一个已安装的模型，接下来就可以愉快的跟ai聊天了！</p><p><img src="https://imgoss.xgss.net/picgo/image-20250206172954722.png?aliyun" alt="image-20250206172954722"></p><h1 id="结束语" tabindex="-1"><a class="header-anchor" href="#结束语"><span>结束语</span></a></h1><p>从环境准备到获取模型文件，再到加载并运行模型，整个过程清晰明了，适合初学者和有一定经验的开发者。</p><p>使用Ollama在本地搭建DeepSeek具有充分利用本地算力、保护数据隐私、便捷更新模型等优点，但同时也存在硬件要求高、技术门槛高、部署过程繁琐等缺点。用户在选择是否进行本地部署时，应根据自己的实际需求和技术水平进行权衡。</p><p>写文不易，如果你都看到了这里，请点个赞和在看，分享给更多的朋友；也别忘了关注星哥玩云！这里有满满的干货分享，还有轻松有趣的技术交流～点个赞、分享给身边的小伙伴，一起成长，一起玩转技术世界吧！ 😊</p>`,125)]))}const o=a(t,[["render",n]]),g=JSON.parse('{"path":"/article/fgzuwugq/","title":"26.简单3步部署本地国产大模型DeepSeek大模型","lang":"en-US","frontmatter":{"title":"26.简单3步部署本地国产大模型DeepSeek大模型","createTime":"2025/05/27 17:51:17","permalink":"/article/fgzuwugq/"},"git":{"createdTime":1749111496000,"updatedTime":1750129445000,"contributors":[{"name":"star","username":"star","email":"star@xgss.net","commits":2,"url":"https://github.com/star"}]},"readingTime":{"minutes":11.65,"words":3496},"filePathRelative":"chatgpt/26.简单3步部署本地国产大模型DeepSeek大模型.md"}');export{o as comp,g as data};
