import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a as p,o as i}from"./app-BiQR_lPj.js";const n={};function o(r,e){return i(),t("div",null,e[0]||(e[0]=[p('<h1 id="deepseek的开源周-开源了那些开源项目-有哪些黑科技" tabindex="-1"><a class="header-anchor" href="#deepseek的开源周-开源了那些开源项目-有哪些黑科技"><span>DeepSeek的开源周，开源了那些开源项目？有哪些黑科技</span></a></h1><h2 id="第一天-flashmla" tabindex="-1"><a class="header-anchor" href="#第一天-flashmla"><span>第一天-FlashMLA</span></a></h2><p>Day 1 of #OpenSourceWeek: FlashMLA</p><p>Honored to share FlashMLA - our efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences and now in production.</p><p>✅ BF16 support ✅ Paged KV cache (block size 64) ⚡ 3000 GB/s memory-bound &amp; 580 TFLOPS compute-bound on H800</p><p>🔗 Explore on GitHub: https://github.com/deepseek-ai/FlashMLA</p><h2 id="第二天-deepep" tabindex="-1"><a class="header-anchor" href="#第二天-deepep"><span>第二天-DeepEP</span></a></h2><p>Day 2 of #OpenSourceWeek: DeepEP</p><p>Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.</p><p>✅ Efficient and optimized all-to-all communication ✅ Both intranode and internode support with NVLink and RDMA ✅ High-throughput kernels for training and inference prefilling ✅ Low-latency kernels for inference decoding ✅ Native FP8 dispatch support ✅ Flexible GPU resource control for computation-communication overlapping</p><p>🔗 GitHub: https://github.com/deepseek-ai/DeepEP</p><h2 id="第三天-deepgemm" tabindex="-1"><a class="header-anchor" href="#第三天-deepgemm"><span>第三天-DeepGEMM</span></a></h2><p>🚀 Day 3 of #OpenSourceWeek: DeepGEMM</p><p>Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.</p><p>⚡ Up to 1350+ FP8 TFLOPS on Hopper GPUs ✅ No heavy dependency, as clean as a tutorial ✅ Fully Just-In-Time compiled ✅ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes ✅ Supports dense layout and two MoE layouts</p><p>🔗 GitHub: https://github.com/deepseek-ai/DeepGEMM</p><h2 id="第四天-optimized-parallelism-strategies" tabindex="-1"><a class="header-anchor" href="#第四天-optimized-parallelism-strategies"><span>第四天-Optimized Parallelism Strategies</span></a></h2><p>🚀 Day 4 of #OpenSourceWeek: Optimized Parallelism Strategies</p><p>✅ DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. 🔗 https://github.com/deepseek-ai/DualPipe</p><p>✅ EPLB - an expert-parallel load balancer for V3/R1. 🔗 https://github.com/deepseek-ai/eplb</p><p>📊 Analyze computation-communication overlap in V3/R1. 🔗 https://github.com/deepseek-ai/profile-data</p><h2 id="第五天-3fs-thruster-for-all-deepseek-data-access" tabindex="-1"><a class="header-anchor" href="#第五天-3fs-thruster-for-all-deepseek-data-access"><span>第五天-3FS, Thruster for All DeepSeek Data Access</span></a></h2><p>🚀 Day 5 of #OpenSourceWeek: 3FS, Thruster for All DeepSeek Data Access</p><p>Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.</p><p>⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster ⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster ⚡ 40+ GiB/s peak throughput per client node for KVCache lookup 🧬 Disaggregated architecture with strong consistency semantics ✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp; KVCache lookups for inference in V3/R1</p><p>📥 3FS → https://github.com/deepseek-ai/3FS ⛲ Smallpond - data processing framework on 3FS → https://github.com/deepseek-ai/smallpond</p><h2 id="第六天-deepseek-v3-r1-inference-system-overview" tabindex="-1"><a class="header-anchor" href="#第六天-deepseek-v3-r1-inference-system-overview"><span>第六天-DeepSeek-V3/R1 Inference System Overview</span></a></h2><p>Day 6 of #OpenSourceWeek: One More Thing – DeepSeek-V3/R1 Inference System Overview</p><p>Optimized throughput and latency via: 🔧 Cross-node EP-powered batch scaling 🔄 Computation-communication overlap ⚖️ Load balancing</p><p>Statistics of DeepSeek&#39;s Online Service: ⚡ 73.7k/14.8k input/output tokens per second per H800 node 🚀 Cost profit margin 545%</p><p>💡 We hope this week&#39;s insights offer value to the community and contribute to our shared AGI goals.</p>',31)]))}const l=a(n,[["render",o]]),d=JSON.parse('{"path":"/article/dufmgpdp/","title":"2.DeepSeek的开源周，开源了那些开源项目？有哪些黑科技","lang":"en-US","frontmatter":{"title":"2.DeepSeek的开源周，开源了那些开源项目？有哪些黑科技","createTime":"2025/05/27 17:51:17","permalink":"/article/dufmgpdp/"},"git":{"createdTime":1749111496000,"updatedTime":1750129445000,"contributors":[{"name":"star","username":"star","email":"star@xgss.net","commits":2,"url":"https://github.com/star"}]},"readingTime":{"minutes":1.59,"words":476},"filePathRelative":"chatgpt2025/2.DeepSeek的开源周，开源了那些开源项目？有哪些黑科技.md"}');export{l as comp,d as data};
