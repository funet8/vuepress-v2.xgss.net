import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a as p,o as i}from"./app-BiQR_lPj.js";const n={};function o(r,e){return i(),t("div",null,e[0]||(e[0]=[p('<h1 id="deepseekçš„å¼€æºå‘¨-å¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®-æœ‰å“ªäº›é»‘ç§‘æŠ€" tabindex="-1"><a class="header-anchor" href="#deepseekçš„å¼€æºå‘¨-å¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®-æœ‰å“ªäº›é»‘ç§‘æŠ€"><span>DeepSeekçš„å¼€æºå‘¨ï¼Œå¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®ï¼Ÿæœ‰å“ªäº›é»‘ç§‘æŠ€</span></a></h1><h2 id="ç¬¬ä¸€å¤©-flashmla" tabindex="-1"><a class="header-anchor" href="#ç¬¬ä¸€å¤©-flashmla"><span>ç¬¬ä¸€å¤©-FlashMLA</span></a></h2><p>Day 1 of #OpenSourceWeek: FlashMLA</p><p>Honored to share FlashMLA - our efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences and now in production.</p><p>âœ… BF16 support âœ… Paged KV cache (block size 64) âš¡ 3000 GB/s memory-bound &amp; 580 TFLOPS compute-bound on H800</p><p>ğŸ”— Explore on GitHub: https://github.com/deepseek-ai/FlashMLA</p><h2 id="ç¬¬äºŒå¤©-deepep" tabindex="-1"><a class="header-anchor" href="#ç¬¬äºŒå¤©-deepep"><span>ç¬¬äºŒå¤©-DeepEP</span></a></h2><p>Day 2 of #OpenSourceWeek: DeepEP</p><p>Excited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference.</p><p>âœ… Efficient and optimized all-to-all communication âœ… Both intranode and internode support with NVLink and RDMA âœ… High-throughput kernels for training and inference prefilling âœ… Low-latency kernels for inference decoding âœ… Native FP8 dispatch support âœ… Flexible GPU resource control for computation-communication overlapping</p><p>ğŸ”— GitHub: https://github.com/deepseek-ai/DeepEP</p><h2 id="ç¬¬ä¸‰å¤©-deepgemm" tabindex="-1"><a class="header-anchor" href="#ç¬¬ä¸‰å¤©-deepgemm"><span>ç¬¬ä¸‰å¤©-DeepGEMM</span></a></h2><p>ğŸš€ Day 3 of #OpenSourceWeek: DeepGEMM</p><p>Introducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.</p><p>âš¡ Up to 1350+ FP8 TFLOPS on Hopper GPUs âœ… No heavy dependency, as clean as a tutorial âœ… Fully Just-In-Time compiled âœ… Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes âœ… Supports dense layout and two MoE layouts</p><p>ğŸ”— GitHub: https://github.com/deepseek-ai/DeepGEMM</p><h2 id="ç¬¬å››å¤©-optimized-parallelism-strategies" tabindex="-1"><a class="header-anchor" href="#ç¬¬å››å¤©-optimized-parallelism-strategies"><span>ç¬¬å››å¤©-Optimized Parallelism Strategies</span></a></h2><p>ğŸš€ Day 4 of #OpenSourceWeek: Optimized Parallelism Strategies</p><p>âœ… DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. ğŸ”— https://github.com/deepseek-ai/DualPipe</p><p>âœ… EPLB - an expert-parallel load balancer for V3/R1. ğŸ”— https://github.com/deepseek-ai/eplb</p><p>ğŸ“Š Analyze computation-communication overlap in V3/R1. ğŸ”— https://github.com/deepseek-ai/profile-data</p><h2 id="ç¬¬äº”å¤©-3fs-thruster-for-all-deepseek-data-access" tabindex="-1"><a class="header-anchor" href="#ç¬¬äº”å¤©-3fs-thruster-for-all-deepseek-data-access"><span>ç¬¬äº”å¤©-3FS, Thruster for All DeepSeek Data Access</span></a></h2><p>ğŸš€ Day 5 of #OpenSourceWeek: 3FS, Thruster for All DeepSeek Data Access</p><p>Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.</p><p>âš¡ 6.6 TiB/s aggregate read throughput in a 180-node cluster âš¡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster âš¡ 40+ GiB/s peak throughput per client node for KVCache lookup ğŸ§¬ Disaggregated architecture with strong consistency semantics âœ… Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp; KVCache lookups for inference in V3/R1</p><p>ğŸ“¥ 3FS â†’ https://github.com/deepseek-ai/3FS â›² Smallpond - data processing framework on 3FS â†’ https://github.com/deepseek-ai/smallpond</p><h2 id="ç¬¬å…­å¤©-deepseek-v3-r1-inference-system-overview" tabindex="-1"><a class="header-anchor" href="#ç¬¬å…­å¤©-deepseek-v3-r1-inference-system-overview"><span>ç¬¬å…­å¤©-DeepSeek-V3/R1 Inference System Overview</span></a></h2><p>Day 6 of #OpenSourceWeek: One More Thing â€“ DeepSeek-V3/R1 Inference System Overview</p><p>Optimized throughput and latency via: ğŸ”§ Cross-node EP-powered batch scaling ğŸ”„ Computation-communication overlap âš–ï¸ Load balancing</p><p>Statistics of DeepSeek&#39;s Online Service: âš¡ 73.7k/14.8k input/output tokens per second per H800 node ğŸš€ Cost profit margin 545%</p><p>ğŸ’¡ We hope this week&#39;s insights offer value to the community and contribute to our shared AGI goals.</p>',31)]))}const l=a(n,[["render",o]]),d=JSON.parse('{"path":"/article/dufmgpdp/","title":"2.DeepSeekçš„å¼€æºå‘¨ï¼Œå¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®ï¼Ÿæœ‰å“ªäº›é»‘ç§‘æŠ€","lang":"en-US","frontmatter":{"title":"2.DeepSeekçš„å¼€æºå‘¨ï¼Œå¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®ï¼Ÿæœ‰å“ªäº›é»‘ç§‘æŠ€","createTime":"2025/05/27 17:51:17","permalink":"/article/dufmgpdp/"},"git":{"createdTime":1749111496000,"updatedTime":1750129445000,"contributors":[{"name":"star","username":"star","email":"star@xgss.net","commits":2,"url":"https://github.com/star"}]},"readingTime":{"minutes":1.59,"words":476},"filePathRelative":"chatgpt2025/2.DeepSeekçš„å¼€æºå‘¨ï¼Œå¼€æºäº†é‚£äº›å¼€æºé¡¹ç›®ï¼Ÿæœ‰å“ªäº›é»‘ç§‘æŠ€.md"}');export{l as comp,d as data};
